{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to work on data frames and can handle any outlier or noise records in datasets\n",
    "import numpy as np # to get algebratic opertaion on data\n",
    "import matplotlib.pyplot as plt # to visualize data and cleaning ( i do it to show outputs of my cleaning (my point of veiw)) \n",
    "import seaborn as sns\n",
    "\n",
    "import re # this library work on regular expressions for pattern matching and string manipulation i search alot on it i understand this only\n",
    "import string # this library has ASCII letters, digits, punctuation, and whitespace i use it to get (punctuation) to remove it from data\n",
    "import nltk # library we use to preprocess on text data to make it clean and in standerd form to use it in NLP\n",
    "from nltk.corpus import stopwords # this module we use to remove stopwords\n",
    "from nltk.tokenize import word_tokenize # this module we use it to tokanise text into words\n",
    "from nltk.stem import WordNetLemmatizer # this lemmatizedr get base of each word to make words in his base ( if verb , noun , adverb or adgectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read our csv file using pd.read_csv\n",
    "data = pd.read_csv(r\"D:\\FCDS\\semster 7\\NLP\\Assginments\\Assginment 2\\text classification\\IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to show our records and columns and what this data contain.\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# to know number of records , null values in dataset and datatype of each attribute\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to describe statstical informations about all numeric attributes\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to know number of null values for all records in each column from our dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **preprocessing on text column** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mazen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we will download all necessary recourses to use ntlk\n",
    "# for stopwords and punctuations and wordnet of english language\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaninig(text):\n",
    "    # 1) Convert all charcters in lowercase to make all words in standerd form\n",
    "    # we save it in cleaned text\n",
    "    cleaned_text = text.lower()\n",
    "    \n",
    "    # 2) remove urls \n",
    "    # Define a regex pattern to match URLs (i study regex patterns from datacamp)(course intro_NLP)\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Replace URLs with an empty string \n",
    "    # with this way we remove URLs from our comments\n",
    "    cleaned_text = url_pattern.sub('', cleaned_text)\n",
    "    \n",
    "    # 3) remove special charcters\n",
    "    # Define a regex pattern to match non-alphanumeric characters and spaces\n",
    "    special_char_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "    # Replace special characters with an empty string\n",
    "    # with this way we remove special characters from our comments\n",
    "    cleaned_text = special_char_pattern.sub('', cleaned_text)\n",
    "    \n",
    "    # 4) remove punctuations\n",
    "    # Define a regex pattern to match punctuation characters\n",
    "    # we use library string to get all punctation marks \n",
    "    punctuation_pattern = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "    # Remove punctuation using the regex pattern\n",
    "    cleaned_text =  punctuation_pattern.sub('', cleaned_text)\n",
    "    \n",
    "    # 5) tokanization\n",
    "    # split text in words\n",
    "    # using built-function word_tokenize(text)\n",
    "    # return splitted words\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # 6) remove stopwords\n",
    "    #we save stopwords of english language in varibale called stop_words\n",
    "    #stopwords.words('') take parameter (language you want to get stopwords of it)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # we will save each token without stopwords in each record \n",
    "    # and will save it in cleaned_tokens\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 7) get base of words using (lemmatization)\n",
    "    #take object from WordNetLemmatizer() to get base of each token(word)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #apply lwmatization on each token on all nouns\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word,pos='n') for word in cleaned_tokens]\n",
    "    #apply lwmatization on each lemmatized_tokens on all verbs\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word,pos='v') for word in lemmatized_tokens]\n",
    "    #apply lwmatization on each lemmatized_tokens on all adverbs\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word,pos='r') for word in lemmatized_tokens]\n",
    "    #apply lwmatization on each lemmatized_tokens on all adjectivs\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word,pos='a') for word in lemmatized_tokens]\n",
    "    #return cummulative lemmatized_tokens (words)\n",
    "    lemmatized_sentences = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return lemmatized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleaning function on comment_text\n",
    "data['cleaned_review'] = data['review'].apply(cleaninig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.sentiment.value_counts().plot(kind='pie' ,color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment column to binary values\n",
    "data['encoded_sentiment'] = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>encoded_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mention watch oz episode youll ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production br br film techniq...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stun f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      cleaned_review  encoded_sentiment  \n",
       "0  one reviewer mention watch oz episode youll ho...                  1  \n",
       "1  wonderful little production br br film techniq...                  1  \n",
       "2  think wonderful way spend time hot summer week...                  1  \n",
       "3  basically there family little boy jake think t...                  0  \n",
       "4  petter matteis love time money visually stun f...                  1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as 'cleaned_reviews.csv' with cleaned_review and encoded_sentiment.\n"
     ]
    }
   ],
   "source": [
    "# Filter the required columns\n",
    "filtered_data = data[['cleaned_review', 'encoded_sentiment']]\n",
    "\n",
    "# Save to CSV\n",
    "filtered_data.to_csv('cleaned_reviews.csv', index=False)\n",
    "\n",
    "print(\"File saved as 'cleaned_reviews.csv' with cleaned_review and encoded_sentiment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Path to the folder containing the files\n",
    "DATA_PATH = r\"D:\\FCDS\\semster 7\\NLP\\Assginments\\Assginment 2\\language model\"\n",
    "\n",
    "# File names\n",
    "LINES_FILE = os.path.join(DATA_PATH, \"movie_lines.txt\")\n",
    "CONVERSATIONS_FILE = os.path.join(DATA_PATH, \"movie_conversations.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bassel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "DATA_PATH = r\"G:\\mazen FCDS\\FCDS\\NLP\\Assginment 2\\language model\"\n",
    "LINES_FILE = os.path.join(DATA_PATH, \"movie_lines.txt\")\n",
    "CONVERSATIONS_FILE = os.path.join(DATA_PATH, \"movie_conversations.txt\")\n",
    "\n",
    "# Function to load movie lines\n",
    "def load_lines(file_path):\n",
    "    lines = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) >= 5:\n",
    "                line_id, text = parts[0], parts[4]\n",
    "                lines[line_id] = text\n",
    "    return lines\n",
    "\n",
    "# Function to load movie conversations\n",
    "def load_conversations(file_path, lines):\n",
    "    conversations = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" +++$+++ \")\n",
    "            if len(parts) >= 4:\n",
    "                utterance_ids = eval(parts[3])  # Convert string to list\n",
    "                conversations.append([lines[line_id] for line_id in utterance_ids if line_id in lines])\n",
    "    return conversations\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9?.!]+\", \" \", text)  # Remove special characters\n",
    "    text = re.sub(r\"([?.!])\", r\" \\1 \", text)       # Add space around punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()       # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Load conversations\n",
    "lines = load_lines(LINES_FILE)\n",
    "conversations = load_conversations(CONVERSATIONS_FILE, lines)\n",
    "\n",
    "# Clean each utterance\n",
    "cleaned_conversations = [[clean_text(utterance) for utterance in conv] for conv in conversations]\n",
    "\n",
    "# Download NLTK tokenization resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize conversations\n",
    "def tokenize(conversations):\n",
    "    return [[word_tokenize(utterance) for utterance in conv] for conv in conversations]\n",
    "\n",
    "# Tokenize cleaned conversations\n",
    "tokenized_conversations = tokenize(cleaned_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Build vocabulary function\n",
    "def build_vocab(tokenized_conversations):\n",
    "    counter = Counter()\n",
    "    for conv in tokenized_conversations:\n",
    "        for utterance in conv:\n",
    "            counter.update(utterance)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_vocab = [word for word, _ in counter.most_common()]\n",
    "    \n",
    "    # Add special tokens\n",
    "    vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN] + sorted_vocab\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, idx2word = build_vocab(tokenized_conversations)\n",
    "\n",
    "# Convert sentences to sequences\n",
    "def sentences_to_sequences(conversations, word2idx):\n",
    "    sequences = []\n",
    "    for conv in conversations:\n",
    "        for utterance in conv:\n",
    "            seq = [word2idx.get(word, word2idx[UNK_TOKEN]) for word in utterance]\n",
    "            sequences.append([word2idx[SOS_TOKEN]] + seq + [word2idx[EOS_TOKEN]])\n",
    "    return sequences\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequences(sequences, pad_token, vocab_size, max_length=None):\n",
    "    # Find the maximum length of sequences if not provided\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "    # Pad sequences\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        seq = [min(i, vocab_size - 1) for i in seq[:max_length]] + [pad_token] * max(0, max_length - len(seq))\n",
    "        padded.append(seq)\n",
    "\n",
    "    return padded, max_length\n",
    "\n",
    "sequences = sentences_to_sequences(tokenized_conversations, word2idx)\n",
    "padded_sequences, max_length = pad_sequences(sequences, word2idx[PAD_TOKEN], len(word2idx))\n",
    "\n",
    "# Create input-output pairs\n",
    "def create_input_output_pairs(padded_sequences):\n",
    "    inputs = [seq[:-1] for seq in padded_sequences]\n",
    "    outputs = [seq[1:] for seq in padded_sequences]\n",
    "    return inputs, outputs\n",
    "\n",
    "inputs, outputs = create_input_output_pairs(padded_sequences)\n",
    "\n",
    "# Define a custom collate function\n",
    "def collate_fn(batch):\n",
    "    inputs = [torch.tensor(item[\"input\"], dtype=torch.long) for item in batch]\n",
    "    outputs = [torch.tensor(item[\"output\"], dtype=torch.long) for item in batch]\n",
    "    \n",
    "    # Pad sequences using the modified pad_sequences function\n",
    "    padded_inputs, max_length = pad_sequences([seq.tolist() for seq in inputs], word2idx[\"<PAD>\"], len(word2idx))\n",
    "    padded_outputs, _ = pad_sequences([seq.tolist() for seq in outputs], word2idx[\"<PAD>\"], len(word2idx), max_length)\n",
    "    \n",
    "    # Convert back to tensors\n",
    "    inputs_padded = torch.tensor(padded_inputs, dtype=torch.long)\n",
    "    outputs_padded = torch.tensor(padded_outputs, dtype=torch.long)\n",
    "    \n",
    "    return {\"input\": inputs_padded, \"output\": outputs_padded}\n",
    "\n",
    "# Define a Dataset class\n",
    "class MovieDialogsDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\"input\": self.inputs[index], \"output\": self.outputs[index]}\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "dataset = MovieDialogsDataset(inputs, outputs)\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to G:\\mazen FCDS\\FCDS\\NLP\\Assginment 2\\language model\\preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Paths to save preprocessed data\n",
    "PREPROCESSED_DATA_PATH = os.path.join(DATA_PATH, \"preprocessed_data.pkl\")\n",
    "\n",
    "# Save preprocessed data\n",
    "def save_preprocessed_data(preprocessed_data, file_path):\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        pickle.dump(preprocessed_data, file)\n",
    "    print(f\"Preprocessed data saved to {file_path}\")\n",
    "\n",
    "# Prepare data to save\n",
    "preprocessed_data = {\n",
    "    \"inputs\": inputs,\n",
    "    \"outputs\": outputs,\n",
    "    \"word2idx\": word2idx,\n",
    "    \"idx2word\": idx2word,\n",
    "    \"max_length\": max_length,\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "save_preprocessed_data(preprocessed_data, PREPROCESSED_DATA_PATH)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
